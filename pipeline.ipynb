{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis of Decretum Burchardi: Topic Modeling, linguistic features, and Keyness\n",
        "\n",
        "This notebook performs a comprehensive analysis of the *Decretum Burchardi* based on an end-to-end automation pipeline \n",
        "using a hybrid approach of digital philology tools and modern transformer-based embeddings.\n",
        "\n",
        "**Objectives:**\n",
        "1.  **Linguistic Profiling:** Analyze Morpho-Syntactic features (POS ratios) using `flair`.\n",
        "2.  **Topic Modeling:** Use **BERTopic** combined with pre-calculated **OpenAI Embeddings** to cluster chapters semantically.\n",
        "3.  **Keyword Extraction:** Use **KeyBERT** to identify the most representative terms per chapter.\n",
        "4.  **Dataset Enrichment:** Merge all analysis results back into the Hugging Face dataset.\n",
        "\n",
        "**Models used:**\n",
        "* Embeddings: OpenAI (`text-embedding-3-large`)\n",
        "* Lemmatizer: `mschonhardt/latin-lemmatizer`\n",
        "* POS Tagger: `mschonhardt/latin-pos-tagger`\n"
      ],
      "id": "37fbb0a0c09fb3c4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# !pip install datasets bertopic keybert flair transformers torch numpy pandas scikit-learn matplotlib\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c83a347d8eb45705"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n",
        "Define models, stop words, and check for GPU availability.\n"
      ],
      "id": "05c2269a61d1907e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# NLP Libraries\n",
        "from transformers import pipeline\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from bertopic import BERTopic\n",
        "from keybert import KeyBERT\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Check for GPU\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "device_name = torch.cuda.get_device_name(0) if device == 0 else \"CPU\"\n",
        "print(f\"Using device: {device_name}\")\n",
        "\n",
        "# Configuration\n",
        "DATASET_ID = \"mschonhardt/bdd-ep-openai-embeddings\"\n",
        "MODEL_LEMMATIZER = \"mschonhardt/latin-lemmatizer\"\n",
        "MODEL_POS = \"mschonhardt/latin-pos-tagger\"\n",
        "\n",
        "# Custom Latin Stopwords List (Critical for clean Topics)\n",
        "LATIN_STOPWORDS = [\n",
        "    \"et\", \"in\", \"de\", \"ad\", \"non\", \"ut\", \"cum\", \"per\", \"a\", \"sed\", \"que\", \n",
        "    \"quia\", \"si\", \"ab\", \"ex\", \"unde\", \"sicut\", \"vel\", \"aut\", \"est\", \"sunt\", \n",
        "    \"esse\", \"fuit\", \"sua\", \"suo\", \"suum\", \"eius\", \"enim\", \"ergo\", \"tamen\",\n",
        "    \"quod\", \"qui\", \"quae\", \"hoc\", \"haec\", \"illud\", \"se\", \"ipsum\", \"autem\",\n",
        "    \"tunc\", \"ubi\", \"ibi\", \"nos\", \"vos\", \"me\", \"te\", \"id\", \"hic\", \"ille\"\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8f3d6f29d5d3d50f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n",
        "Load the dataset containing the raw text and the pre-calculated OpenAI embeddings.\n"
      ],
      "id": "e64d74fce0ef8222"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Loading dataset: {DATASET_ID}...\")\n",
        "dataset = load_dataset(DATASET_ID, split=\"train\")\n",
        "\n",
        "# Extract columns\n",
        "texts = dataset[\"text\"]\n",
        "# BERTopic requires numpy arrays for embeddings\n",
        "embeddings = np.array(dataset[\"embedding\"])\n",
        "\n",
        "print(f\"Successfully loaded {len(texts)} chapters.\")\n",
        "print(f\"Embedding shape: {embeddings.shape}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f0f6bb4e52c8b98a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing: Lemmatization\n",
        "Use Seq2Seq transformer model to lemmatize the Latin text. \n",
        "These lemmatized texts will be used for Topic Representation and Keyword extraction, while the OpenAI embeddings are used for Clustering.\n"
      ],
      "id": "1b4687f1e1e5b3ff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Initializing Lemmatizer Pipeline...\")\n",
        "lemmatizer_pipe = pipeline(\"text2text-generation\", model=MODEL_LEMMATIZER, device=device)\n",
        "\n",
        "lemmatized_texts = []\n",
        "batch_size = 32  # Adjust based on your VRAM\n",
        "\n",
        "print(\"Starting Lemmatization (this may take a while)...\")\n",
        "for i in tqdm(range(0, len(texts), batch_size), desc=\"Lemmatizing\"):\n",
        "    batch = texts[i:i+batch_size]\n",
        "    # Truncation ensures we don't crash on extremely long chapters\n",
        "    results = lemmatizer_pipe(batch, max_length=512, truncation=True)\n",
        "    lemmatized_texts.extend([res['generated_text'] for res in results])\n",
        "\n",
        "print(\"Sample Lemmatized Text:\", lemmatized_texts[0][:100])\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6cbafcfd8ea2987c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Linguistic Analysis: POS Tagging\n",
        "Use `flair` to calculate the density of Nouns, Verbs, and Adjectives. \n"
      ],
      "id": "f6b1a92b6f0b6b6e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Loading POS Tagger...\")\n",
        "pos_tagger = SequenceTagger.load(MODEL_POS)\n",
        "\n",
        "pos_stats = []\n",
        "\n",
        "print(\"Analyzing Part-of-Speech ratios...\")\n",
        "for text in tqdm(texts, desc=\"POS Tagging\"):\n",
        "    sentence = Sentence(text)\n",
        "    pos_tagger.predict(sentence)\n",
        "    \n",
        "    # Extract tags\n",
        "    tags = [token.get_label('pos').value for token in sentence]\n",
        "    total_tokens = len(tags)\n",
        "    \n",
        "    if total_tokens > 0:\n",
        "        stats = {\n",
        "            \"noun_ratio\": tags.count('NOUN') / total_tokens,\n",
        "            \"verb_ratio\": tags.count('VERB') / total_tokens,\n",
        "            \"adj_ratio\": tags.count('ADJ') / total_tokens\n",
        "        }\n",
        "    else:\n",
        "        stats = {\"noun_ratio\": 0.0, \"verb_ratio\": 0.0, \"adj_ratio\": 0.0}\n",
        "    \n",
        "    pos_stats.append(stats)\n",
        "\n",
        "# Convert to DataFrame for easy handling later\n",
        "df_pos = pd.DataFrame(pos_stats)\n",
        "print(df_pos.head())\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0cf4f8c9a13c8a55"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Topic Modeling (BERTopic)\n",
        "Combine **Clustering** based on high-quality **OpenAI Embeddings** and **Representation** based on **Lemmatized Texts**.\n"
      ],
      "id": "a62a3c6d9b6c66b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Initializing BERTopic...\")\n",
        "\n",
        "# Use the custom Latin stopwords list\n",
        "vectorizer_model = CountVectorizer(stop_words=LATIN_STOPWORDS)\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    verbose=True,\n",
        "    n_gram_range=(1, 1) # Focus on single words for topic labels\n",
        ")\n",
        "\n",
        "print(\"Fitting BERTopic model...\")\n",
        "# Pass lemmatized text for labels, but embeddings for clustering\n",
        "topics, probs = topic_model.fit_transform(lemmatized_texts, embeddings)\n",
        "\n",
        "# Show top topics\n",
        "print(topic_model.get_topic_info().head(10))\n",
        "\n",
        "# Visualization (Interactive in Jupyter)\n",
        "topic_model.visualize_topics()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8a9c4f3d0d42de6d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Keyword Extraction (KeyBERT)\n",
        "Extract the specific \"Keyness\" of each chapter using KeyBERT.\n",
        "Use a multilingual model that works well with Latin to find words in the text that are semantically closest to the chapter's meaning.\n"
      ],
      "id": "2f6b6b7df4b6d21f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Initializing KeyBERT...\")\n",
        "kw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "chapter_keywords = []\n",
        "\n",
        "print(\"Extracting Keywords per chapter...\")\n",
        "for text in tqdm(lemmatized_texts, desc=\"KeyBERT\"):\n",
        "    kws = kw_model.extract_keywords(\n",
        "        text, \n",
        "        keyphrase_ngram_range=(1, 1), \n",
        "        stop_words=LATIN_STOPWORDS, \n",
        "        top_n=5\n",
        "    )\n",
        "    # Store as comma-separated string\n",
        "    chapter_keywords.append(\", \".join([k[0] for k in kws]))\n",
        "\n",
        "print(f\"Sample Keywords for Chapter 0: {chapter_keywords[0]}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0b79a4b62a1d4b4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Aggregation & Export\n",
        "Combine all new features into a single Hugging Face Dataset object.\n"
      ],
      "id": "b2d6f3be1486d1a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Aggregating data...\")\n",
        "\n",
        "# Create a dictionary of new features\n",
        "new_features = {\n",
        "    \"lemmatized_text\": lemmatized_texts,\n",
        "    \"topic_id\": topics,\n",
        "    # Map topic IDs to their string representation (top 3 words)\n",
        "    \"topic_name\": [topic_model.get_topic(t)[0][0] + \"_\" + topic_model.get_topic(t)[1][0] if t != -1 else \"outlier\" for t in topics],\n",
        "    \"keywords\": chapter_keywords,\n",
        "    \"noun_ratio\": df_pos[\"noun_ratio\"].tolist(),\n",
        "    \"verb_ratio\": df_pos[\"verb_ratio\"].tolist(),\n",
        "    \"adj_ratio\": df_pos[\"adj_ratio\"].tolist()\n",
        "}\n",
        "\n",
        "# Add columns to the existing dataset\n",
        "dataset_enriched = dataset\n",
        "for name, data in new_features.items():\n",
        "    dataset_enriched = dataset_enriched.add_column(name, data)\n",
        "\n",
        "print(\"Enriched Dataset Structure:\")\n",
        "print(dataset_enriched)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6d9b66a5f4c9e3e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving the Data\n",
        "Uncomment the lines below to save locally or push to the Hub.\n"
      ],
      "id": "4b6f42b4f9f15dcb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Option 1: Save locally\n",
        "# dataset_enriched.save_to_disk(\"./burchard_enriched\")\n",
        "\n",
        "# Option 2: Push to Hugging Face Hub\n",
        "# NOTE: You need to be logged in via `huggingface-cli login`\n",
        "# dataset_enriched.push_to_hub(\"YOUR_USERNAME/bdd-ep-analysed\")\n",
        "\n",
        "print(\"Analysis Complete.\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4d5b19a2db5f08a4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "nbconvert_exporter": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
